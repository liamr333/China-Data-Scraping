{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6d019853-0a2c-44eb-9b77-13064d692c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromedriver-py\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3c4856ee-8fa3-4524-b437-9dba85bcb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver as ChromeWebDriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import requests\n",
    "from typing import List, Dict, Union, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "35fa26c4-6ee4-416f-a2d9-eba22a11c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_titles_output_file = \"global_times_relevant_articles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "952816aa-b0ce-4c8f-8d2a-06d5915907ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_write_dir = \"data\"\n",
    "\n",
    "if all_write_dir not in os.listdir():\n",
    "    os.mkdir(f\"./{all_write_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6259a0bb-bce2-4b94-99d3-b3673b2d9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dir = \"global_times\"\n",
    "\n",
    "if sub_dir not in os.listdir(all_write_dir):\n",
    "    os.mkdir(f\"./{all_write_dir}/{sub_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "320ce81d-d8b6-4d1c-9859-71f162cd6d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chrome_driver(headless: bool=False) -> ChromeWebDriver:\n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d2f6a563-4c35-4af5-a969-af4c30637145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using advanced search, we can see that articles from the Global Times start at 2008/09/28.\n",
    "# Let's search month by month, setting the begin_date and end_date to the first and last date of that month respectively,\n",
    "# For every month from 2008/09 to 2025/08\n",
    "\n",
    "def get_first_last_days_of_months(\n",
    "    start_date: Union[str, date, datetime],\n",
    "    end_date: Union[str, date, datetime]\n",
    ") -> List[Dict[str, Union[str, date]]]:\n",
    "    \"\"\"Get first and last day of each month in range using pandas\"\"\"\n",
    "    \n",
    "    # Create date range of all months\n",
    "    months = pd.date_range(start=start_date, end=end_date, freq='MS')  # Month Start\n",
    "    \n",
    "    results = []\n",
    "    for month_start in months:\n",
    "        # First day is the month start\n",
    "        first_day = month_start\n",
    "        \n",
    "        # Last day is the last day of that month\n",
    "        last_day = month_start + pd.offsets.MonthEnd(0)\n",
    "        \n",
    "        results.append({\n",
    "            'month': month_start.strftime('%Y-%m'),\n",
    "            'first_day': first_day.date(),\n",
    "            'last_day': last_day.date()\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9b719e1b-7241-4ddf-abd5-ae57a42de73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(prompt: str, model=\"llama2:7b\") -> str:\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data)\n",
    "    \n",
    "    # Add error checking\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error: {response.json()}\"\n",
    "    \n",
    "    return response.json()[\"response\"].strip()\n",
    "\n",
    "\"\"\"\n",
    "each returned row is formatted like [date (of article), article title, article url]\n",
    "\"\"\"\n",
    "def get_article_data(month: str, page_source: BeautifulSoup) -> List[List[str]]:\n",
    "    article_data = []\n",
    "    \n",
    "    for bc in soup.find_all(\"blockquote\"):\n",
    "        try:\n",
    "            article_date = re.findall(r\"^\\d{2,4}/\\d{2}/\\d{2}\", bc.small.text.strip())[0]\n",
    "            article_title = bc.a.text.strip()\n",
    "            article_link = bc.a.get(\"href\")\n",
    "            article_data.append([month, article_date, article_title, article_link])\n",
    "        except Exception as e:\n",
    "            print(f\"Insufficient article data - {e}\")\n",
    "    \n",
    "    return article_data\n",
    "\n",
    "\n",
    "def get_relevant_articles(article_titles: List[str]) -> List[str]:\n",
    "    # remove pipes if they exist in article titles\n",
    "    article_titles = [article_title.replace(\"|\", \"\") for article_title in article_titles]\n",
    "    prompt = \"\"\"Hey Gemma, here are some article titles separated by a pipe. \n",
    "    Please return a pipe-separated list of which ones correspond to China. \n",
    "    Only return the pipe-separated list\n",
    "    and nothing else. It should be formatted like <article_title>|<article_title>|<article_title>.\n",
    "    If an article is not related to China, don't include it in the\n",
    "    list or in the output at all.\n",
    "    Here are the article links: {}\"\"\".format(\"|\".join(article_titles))\n",
    "    print(prompt)\n",
    "    result = query_model(prompt, model=\"gemma3:12b\")\n",
    "    relevant_article_titles = []\n",
    "    if not re.search(\"^Error\", result):\n",
    "        print(result)\n",
    "        relevant_article_titles.extend(result.split(\"|\"))\n",
    "    else:\n",
    "        print(result)\n",
    "    return relevant_article_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f9a5e723-e7c1-48ee-8797-6e85cd502a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "each row in data should be formatted like \n",
    "[month, date (of article), article title, article url]\n",
    "\"\"\"\n",
    "def append_article_data(\n",
    "                        output_file: str,\n",
    "                        data: List[List[str]]\n",
    "                       ) -> None:\n",
    "    if output_file not in os.listdir():\n",
    "        with open(output_file, \"w+\", encoding=\"cp1252\", newline=\"\") as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            headers = [\"month\", \"date\", \"article_title\", \"article_link\"]\n",
    "            csv_file.writerow(headers)\n",
    "\n",
    "    with open(output_file, \"a\", encoding=\"cp1252\", newline=\"\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerows(data)\n",
    "\n",
    "\n",
    "# search for all Global Times articles within a month range\n",
    "def month_search(month: str, begin_date: str, end_date: str) -> None:\n",
    "    driver = create_chrome_driver()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    try:\n",
    "        driver.get(\"https://search.globaltimes.cn/SearchCtrl\")\n",
    "        \n",
    "        begin_date_input = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"begin_date\"]'))\n",
    "        )\n",
    "        end_date_input = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"end_date\"]'))\n",
    "        )\n",
    "        driver.execute_script(f\"arguments[0].setAttribute('value', '{begin_date}')\", begin_date_input)\n",
    "        driver.execute_script(f\"arguments[0].setAttribute('value', '{end_date}')\", end_date_input)\n",
    "        \n",
    "        search_button = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[3]/div/div/form/div[8]'))\n",
    "        )\n",
    "        search_button.click()\n",
    "        \n",
    "        time.sleep(10)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        num_articles_element = wait.until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[4]/div[1]/div[12]/font'))\n",
    "        )\n",
    "        num_articles = 0\n",
    "        if re.search(r'\\d+', num_articles_element.text):\n",
    "            num_articles = int(re.findall(r'\\d+', num_articles_element.text)[0])\n",
    "\n",
    "        for page in range(num_articles):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            page_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            page_article_data = get_article_data(month, page_soup)\n",
    "            page_article_titles = [row[2] for row in page_article_data]\n",
    "            page_relevant_article_titles = get_relevant_articles(page_article_titles)\n",
    "            relevant_article_data = [row for row in page_article_data if row[2] in page_relevant_article_titles]\n",
    "\n",
    "            append_article_data(relevant_titles_output_file, relevant_article_data)            \n",
    "            \n",
    "            all_buttons = driver.find_elements(By.CLASS_NAME, \"btn\")\n",
    "            if len(all_buttons) > 0:\n",
    "                next_button = all_buttons[0]\n",
    "                for button in all_buttons:\n",
    "                    if 'Next' in button.get_attribute(\"innerHTML\"):\n",
    "                        next_button = button\n",
    "                        next_page_script = next_button.get_attribute(\"onclick\")\n",
    "                        driver.execute_script(next_page_script)\n",
    "                        break\n",
    "            \n",
    "            \n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout waiting for elements for date range {begin_date} to {end_date}\")\n",
    "    except NoSuchElementException as e:\n",
    "        print(f\"Element not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900122a-48b5-4aab-a3cb-25a0f71e6d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
